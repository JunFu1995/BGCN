2023-12-20 13:27:29,275 - INFO - Log directory: ./libcity/log
2023-12-20 13:27:29,275 - INFO - Begin pipeline, task=traffic_state_pred, model_name=BGCN, dataset_name=PeMS04, exp_id=48492
2023-12-20 13:27:29,275 - INFO - {'task': 'traffic_state_pred', 'model': 'BGCN', 'dataset': 'PeMS04', 'saved_model': True, 'train': True, 'seed': 0, 'dataset_class': 'TrafficStatePointDataset', 'executor': 'TrafficStateExecutor', 'evaluator': 'TrafficStateEvaluator', 'dropout': 0.3, 'blocks': 4, 'layers': 2, 'apt_layer': True, 'gcn_bool': True, 'addaptadj': True, 'adjtype': 'doubletransition', 'bidir_adj_mx': False, 'randomadj': True, 'aptonly': True, 'kernel_size': 2, 'nhid': 32, 'residual_channels': 32, 'dilation_channels': 32, 'skip_channels': 256, 'end_channels': 512, 'scaler': 'standard', 'load_external': False, 'normal_external': False, 'ext_scaler': 'none', 'add_time_in_day': False, 'add_day_in_week': False, 'max_epoch': 100, 'learner': 'adam', 'learning_rate': 0.001, 'lr_decay': False, 'clip_grad_norm': True, 'max_grad_norm': 5, 'use_early_stop': False, 'batch_size': 64, 'cache_dataset': True, 'num_workers': 0, 'pad_with_last_sample': True, 'train_rate': 0.6, 'eval_rate': 0.2, 'input_window': 12, 'output_window': 12, 'gpu': True, 'gpu_id': 0, 'train_loss': 'none', 'epoch': 0, 'weight_decay': 0, 'lr_epsilon': 1e-08, 'lr_beta1': 0.9, 'lr_beta2': 0.999, 'lr_alpha': 0.99, 'lr_momentum': 0, 'lr_scheduler': 'multisteplr', 'lr_decay_ratio': 0.1, 'steps': [5, 20, 40, 70], 'step_size': 10, 'lr_T_max': 30, 'lr_eta_min': 0, 'lr_patience': 10, 'lr_threshold': 0.0001, 'patience': 50, 'log_level': 'INFO', 'log_every': 1, 'load_best_epoch': True, 'hyper_tune': False, 'metrics': ['MAE', 'MAPE', 'MSE', 'RMSE', 'masked_MAE', 'masked_MAPE', 'masked_MSE', 'masked_RMSE', 'R2', 'EVAR'], 'evaluator_mode': 'single', 'save_mode': ['csv'], 'geo': {'including_types': ['Point'], 'Point': {}}, 'rel': {'including_types': ['geo'], 'geo': {'cost': 'num'}}, 'dyna': {'including_types': ['state'], 'state': {'entity_id': 'geo_id', 'traffic_flow': 'num', 'traffic_occupancy': 'num', 'traffic_speed': 'num'}}, 'data_col': ['traffic_flow'], 'weight_col': 'cost', 'data_files': ['PeMS04'], 'geo_file': 'PeMS04', 'rel_file': 'PeMS04', 'output_dim': 1, 'time_intervals': 300, 'init_weight_inf_or_zero': 'zero', 'set_weight_link_or_dist': 'link', 'calculate_weight_adj': False, 'weight_adj_epsilon': 0, 'device': device(type='cuda', index=0), 'exp_id': 48492}
2023-12-20 13:27:29,278 - INFO - Loaded file PeMS04.geo, num_nodes=307
2023-12-20 13:27:29,279 - INFO - set_weight_link_or_dist: link
2023-12-20 13:27:29,279 - INFO - init_weight_inf_or_zero: zero
2023-12-20 13:27:29,280 - INFO - Loaded file PeMS04.rel, shape=(307, 307)
2023-12-20 13:27:29,280 - INFO - Loading file PeMS04.dyna
2023-12-20 13:27:30,671 - INFO - Loaded file PeMS04.dyna, shape=(16992, 307, 1)
2023-12-20 13:27:31,197 - INFO - Dataset created
2023-12-20 13:27:31,197 - INFO - x shape: (16969, 12, 307, 1), y shape: (16969, 12, 307, 1)
2023-12-20 13:27:31,198 - INFO - train	x: (10181, 12, 307, 1), y: (10181, 12, 307, 1)
2023-12-20 13:27:31,198 - INFO - eval	x: (3394, 12, 307, 1), y: (3394, 12, 307, 1)
2023-12-20 13:27:31,198 - INFO - test	x: (3394, 12, 307, 1), y: (3394, 12, 307, 1)
2023-12-20 13:27:37,769 - INFO - Saved at ./libcity/cache/dataset_cache/point_based_PeMS04_12_12_0.6_0.2_standard_64_False_False_False_True.npz
2023-12-20 13:27:37,847 - INFO - StandardScaler mean: 207.22733840505313, std: 156.47765518492758
2023-12-20 13:27:37,847 - INFO - NoneScaler
2023-12-20 13:27:38,370 - INFO - receptive_field: 13
2023-12-20 13:27:38,489 - INFO - BGCN(
  (filter_convs): ModuleList(
    (0): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1))
    (1): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1), dilation=(2, 2))
    (2): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1))
    (3): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1), dilation=(2, 2))
    (4): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1))
    (5): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1), dilation=(2, 2))
    (6): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1))
    (7): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1), dilation=(2, 2))
  )
  (gate_convs): ModuleList(
    (0): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1))
    (1): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1), dilation=(2, 2))
    (2): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1))
    (3): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1), dilation=(2, 2))
    (4): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1))
    (5): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1), dilation=(2, 2))
    (6): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1))
    (7): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1), dilation=(2, 2))
  )
  (residual_convs): ModuleList(
    (0-7): 8 x Conv1d(32, 32, kernel_size=(1, 1), stride=(1,))
  )
  (skip_convs): ModuleList(
    (0-7): 8 x Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1))
  )
  (bn): ModuleList(
    (0-7): 8 x BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (gconv): ModuleList(
    (0-7): 8 x GCN(
      (nconv): NConv()
      (mlp): Linear(
        (mlp): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
  (start_conv): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1))
  (end_conv_1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
  (end_conv_2): Conv2d(512, 12, kernel_size=(1, 1), stride=(1, 1))
)
2023-12-20 13:27:38,489 - INFO - PA	torch.Size([307, 307])	cuda:0	True
2023-12-20 13:27:38,489 - INFO - filter_convs.0.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 13:27:38,489 - INFO - filter_convs.0.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,489 - INFO - filter_convs.1.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 13:27:38,489 - INFO - filter_convs.1.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,489 - INFO - filter_convs.2.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 13:27:38,489 - INFO - filter_convs.2.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,489 - INFO - filter_convs.3.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 13:27:38,489 - INFO - filter_convs.3.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,489 - INFO - filter_convs.4.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 13:27:38,489 - INFO - filter_convs.4.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,489 - INFO - filter_convs.5.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 13:27:38,489 - INFO - filter_convs.5.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,489 - INFO - filter_convs.6.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 13:27:38,489 - INFO - filter_convs.6.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - filter_convs.7.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - filter_convs.7.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - gate_convs.0.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - gate_convs.0.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - gate_convs.1.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - gate_convs.1.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - gate_convs.2.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - gate_convs.2.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - gate_convs.3.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - gate_convs.3.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - gate_convs.4.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - gate_convs.4.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - gate_convs.5.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - gate_convs.5.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - gate_convs.6.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - gate_convs.6.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - gate_convs.7.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - gate_convs.7.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - residual_convs.0.weight	torch.Size([32, 32, 1, 1])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - residual_convs.0.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - residual_convs.1.weight	torch.Size([32, 32, 1, 1])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - residual_convs.1.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - residual_convs.2.weight	torch.Size([32, 32, 1, 1])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - residual_convs.2.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - residual_convs.3.weight	torch.Size([32, 32, 1, 1])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - residual_convs.3.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - residual_convs.4.weight	torch.Size([32, 32, 1, 1])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - residual_convs.4.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - residual_convs.5.weight	torch.Size([32, 32, 1, 1])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - residual_convs.5.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - residual_convs.6.weight	torch.Size([32, 32, 1, 1])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - residual_convs.6.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - residual_convs.7.weight	torch.Size([32, 32, 1, 1])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - residual_convs.7.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - skip_convs.0.weight	torch.Size([256, 32, 1, 1])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - skip_convs.0.bias	torch.Size([256])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - skip_convs.1.weight	torch.Size([256, 32, 1, 1])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - skip_convs.1.bias	torch.Size([256])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - skip_convs.2.weight	torch.Size([256, 32, 1, 1])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - skip_convs.2.bias	torch.Size([256])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - skip_convs.3.weight	torch.Size([256, 32, 1, 1])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - skip_convs.3.bias	torch.Size([256])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - skip_convs.4.weight	torch.Size([256, 32, 1, 1])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - skip_convs.4.bias	torch.Size([256])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - skip_convs.5.weight	torch.Size([256, 32, 1, 1])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - skip_convs.5.bias	torch.Size([256])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - skip_convs.6.weight	torch.Size([256, 32, 1, 1])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - skip_convs.6.bias	torch.Size([256])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - skip_convs.7.weight	torch.Size([256, 32, 1, 1])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - skip_convs.7.bias	torch.Size([256])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - bn.0.weight	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - bn.0.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - bn.1.weight	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - bn.1.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - bn.2.weight	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - bn.2.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - bn.3.weight	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - bn.3.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - bn.4.weight	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,490 - INFO - bn.4.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - bn.5.weight	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - bn.5.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - bn.6.weight	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - bn.6.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - bn.7.weight	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - bn.7.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - gconv.0.mlp.mlp.weight	torch.Size([32, 32, 1, 1])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - gconv.0.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - gconv.1.mlp.mlp.weight	torch.Size([32, 32, 1, 1])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - gconv.1.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - gconv.2.mlp.mlp.weight	torch.Size([32, 32, 1, 1])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - gconv.2.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - gconv.3.mlp.mlp.weight	torch.Size([32, 32, 1, 1])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - gconv.3.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - gconv.4.mlp.mlp.weight	torch.Size([32, 32, 1, 1])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - gconv.4.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - gconv.5.mlp.mlp.weight	torch.Size([32, 32, 1, 1])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - gconv.5.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - gconv.6.mlp.mlp.weight	torch.Size([32, 32, 1, 1])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - gconv.6.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - gconv.7.mlp.mlp.weight	torch.Size([32, 32, 1, 1])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - gconv.7.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - start_conv.weight	torch.Size([32, 1, 1, 1])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - start_conv.bias	torch.Size([32])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - end_conv_1.weight	torch.Size([512, 256, 1, 1])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - end_conv_1.bias	torch.Size([512])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - end_conv_2.weight	torch.Size([12, 512, 1, 1])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - end_conv_2.bias	torch.Size([12])	cuda:0	True
2023-12-20 13:27:38,491 - INFO - Total parameter numbers: 350325
2023-12-20 13:27:38,491 - INFO - You select `adam` optimizer.
2023-12-20 13:27:38,491 - WARNING - Received none train loss func and will use the loss func defined in the model.
2023-12-20 13:27:38,491 - INFO - Start training ...
2023-12-20 13:27:38,491 - INFO - num_batches:160
2023-12-20 13:28:10,120 - INFO - epoch complete!
2023-12-20 13:28:10,120 - INFO - evaluating now!
2023-12-20 13:28:18,046 - INFO - Epoch [0/100] train_loss: 34.3428, val_loss: 28.8482, lr: 0.001000, 39.55s
2023-12-20 13:28:18,058 - INFO - Saved model at 0
2023-12-20 13:28:18,058 - INFO - Val loss decrease from inf to 28.8482, saving to ./libcity/cache/48492/model_cache/BGCN_PeMS04_epoch0.tar
2023-12-20 13:28:49,429 - INFO - epoch complete!
2023-12-20 13:28:49,430 - INFO - evaluating now!
2023-12-20 13:28:57,536 - INFO - Epoch [1/100] train_loss: 26.3091, val_loss: 29.0979, lr: 0.001000, 39.48s
2023-12-20 13:29:30,082 - INFO - epoch complete!
2023-12-20 13:29:30,082 - INFO - evaluating now!
2023-12-20 13:29:38,040 - INFO - Epoch [2/100] train_loss: 24.0395, val_loss: 23.6271, lr: 0.001000, 40.50s
2023-12-20 13:29:38,051 - INFO - Saved model at 2
2023-12-20 13:29:38,051 - INFO - Val loss decrease from 28.8482 to 23.6271, saving to ./libcity/cache/48492/model_cache/BGCN_PeMS04_epoch2.tar
2023-12-20 13:30:09,923 - INFO - epoch complete!
2023-12-20 13:30:09,923 - INFO - evaluating now!
2023-12-20 13:30:17,833 - INFO - Epoch [3/100] train_loss: 23.1472, val_loss: 22.9675, lr: 0.001000, 39.78s
2023-12-20 13:30:17,845 - INFO - Saved model at 3
2023-12-20 13:30:17,845 - INFO - Val loss decrease from 23.6271 to 22.9675, saving to ./libcity/cache/48492/model_cache/BGCN_PeMS04_epoch3.tar
2023-12-20 13:30:49,003 - INFO - epoch complete!
2023-12-20 13:30:49,004 - INFO - evaluating now!
2023-12-20 13:30:56,909 - INFO - Epoch [4/100] train_loss: 22.2183, val_loss: 23.0838, lr: 0.001000, 39.06s
2023-12-20 13:31:28,107 - INFO - epoch complete!
2023-12-20 13:31:28,107 - INFO - evaluating now!
2023-12-20 13:31:36,003 - INFO - Epoch [5/100] train_loss: 21.7864, val_loss: 21.3716, lr: 0.001000, 39.09s
2023-12-20 13:31:36,015 - INFO - Saved model at 5
2023-12-20 13:31:36,015 - INFO - Val loss decrease from 22.9675 to 21.3716, saving to ./libcity/cache/48492/model_cache/BGCN_PeMS04_epoch5.tar
2023-12-20 13:32:07,014 - INFO - epoch complete!
2023-12-20 13:32:07,014 - INFO - evaluating now!
2023-12-20 13:32:14,925 - INFO - Epoch [6/100] train_loss: 21.2912, val_loss: 21.4754, lr: 0.001000, 38.91s
2023-12-20 13:32:46,137 - INFO - epoch complete!
2023-12-20 13:32:46,137 - INFO - evaluating now!
2023-12-20 13:32:54,063 - INFO - Epoch [7/100] train_loss: 21.0332, val_loss: 21.6089, lr: 0.001000, 39.14s
2023-12-20 13:33:25,225 - INFO - epoch complete!
2023-12-20 13:33:25,225 - INFO - evaluating now!
2023-12-20 13:33:33,152 - INFO - Epoch [8/100] train_loss: 20.9032, val_loss: 20.3004, lr: 0.001000, 39.09s
2023-12-20 13:33:33,164 - INFO - Saved model at 8
2023-12-20 13:33:33,164 - INFO - Val loss decrease from 21.3716 to 20.3004, saving to ./libcity/cache/48492/model_cache/BGCN_PeMS04_epoch8.tar
2023-12-20 13:34:04,362 - INFO - epoch complete!
2023-12-20 13:34:04,362 - INFO - evaluating now!
2023-12-20 13:34:12,241 - INFO - Epoch [9/100] train_loss: 20.5679, val_loss: 20.8982, lr: 0.001000, 39.08s
2023-12-20 13:34:43,239 - INFO - epoch complete!
2023-12-20 13:34:43,239 - INFO - evaluating now!
2023-12-20 13:34:51,269 - INFO - Epoch [10/100] train_loss: 20.4418, val_loss: 20.6788, lr: 0.001000, 39.03s
2023-12-20 13:35:22,691 - INFO - epoch complete!
2023-12-20 13:35:22,691 - INFO - evaluating now!
2023-12-20 13:35:30,513 - INFO - Epoch [11/100] train_loss: 20.3516, val_loss: 20.2664, lr: 0.001000, 39.24s
2023-12-20 13:35:30,524 - INFO - Saved model at 11
2023-12-20 13:35:30,524 - INFO - Val loss decrease from 20.3004 to 20.2664, saving to ./libcity/cache/48492/model_cache/BGCN_PeMS04_epoch11.tar
2023-12-20 13:36:01,456 - INFO - epoch complete!
2023-12-20 13:36:01,456 - INFO - evaluating now!
2023-12-20 13:36:09,269 - INFO - Epoch [12/100] train_loss: 20.1707, val_loss: 20.4347, lr: 0.001000, 38.74s
2023-12-20 13:36:40,210 - INFO - epoch complete!
2023-12-20 13:36:40,210 - INFO - evaluating now!
2023-12-20 13:36:48,036 - INFO - Epoch [13/100] train_loss: 20.3303, val_loss: 20.6908, lr: 0.001000, 38.77s
2023-12-20 13:37:18,940 - INFO - epoch complete!
2023-12-20 13:37:18,940 - INFO - evaluating now!
2023-12-20 13:37:26,750 - INFO - Epoch [14/100] train_loss: 20.0916, val_loss: 20.3611, lr: 0.001000, 38.71s
2023-12-20 13:37:57,641 - INFO - epoch complete!
2023-12-20 13:37:57,641 - INFO - evaluating now!
2023-12-20 13:38:05,489 - INFO - Epoch [15/100] train_loss: 19.9814, val_loss: 20.4059, lr: 0.001000, 38.74s
2023-12-20 13:38:36,436 - INFO - epoch complete!
2023-12-20 13:38:36,436 - INFO - evaluating now!
2023-12-20 13:38:44,255 - INFO - Epoch [16/100] train_loss: 19.8227, val_loss: 19.8461, lr: 0.001000, 38.77s
2023-12-20 13:38:44,266 - INFO - Saved model at 16
2023-12-20 13:38:44,266 - INFO - Val loss decrease from 20.2664 to 19.8461, saving to ./libcity/cache/48492/model_cache/BGCN_PeMS04_epoch16.tar
2023-12-20 13:39:15,226 - INFO - epoch complete!
2023-12-20 13:39:15,226 - INFO - evaluating now!
2023-12-20 13:39:23,054 - INFO - Epoch [17/100] train_loss: 19.8403, val_loss: 20.5202, lr: 0.001000, 38.79s
2023-12-20 13:39:53,966 - INFO - epoch complete!
2023-12-20 13:39:53,966 - INFO - evaluating now!
2023-12-20 13:40:01,772 - INFO - Epoch [18/100] train_loss: 19.7878, val_loss: 19.7222, lr: 0.001000, 38.72s
2023-12-20 13:40:01,783 - INFO - Saved model at 18
2023-12-20 13:40:01,783 - INFO - Val loss decrease from 19.8461 to 19.7222, saving to ./libcity/cache/48492/model_cache/BGCN_PeMS04_epoch18.tar
2023-12-20 13:40:32,704 - INFO - epoch complete!
2023-12-20 13:40:32,704 - INFO - evaluating now!
2023-12-20 13:40:40,533 - INFO - Epoch [19/100] train_loss: 19.7825, val_loss: 19.9776, lr: 0.001000, 38.75s
2023-12-20 13:41:11,148 - INFO - epoch complete!
2023-12-20 13:41:11,148 - INFO - evaluating now!
2023-12-20 13:41:18,778 - INFO - Epoch [20/100] train_loss: 19.6837, val_loss: 20.1885, lr: 0.001000, 38.25s
2023-12-20 13:41:49,208 - INFO - epoch complete!
2023-12-20 13:41:49,208 - INFO - evaluating now!
2023-12-20 13:41:56,855 - INFO - Epoch [21/100] train_loss: 19.6594, val_loss: 19.6991, lr: 0.001000, 38.08s
2023-12-20 13:41:56,866 - INFO - Saved model at 21
2023-12-20 13:41:56,866 - INFO - Val loss decrease from 19.7222 to 19.6991, saving to ./libcity/cache/48492/model_cache/BGCN_PeMS04_epoch21.tar
2023-12-20 13:42:27,347 - INFO - epoch complete!
2023-12-20 13:42:27,347 - INFO - evaluating now!
2023-12-20 13:42:34,994 - INFO - Epoch [22/100] train_loss: 19.5688, val_loss: 19.4844, lr: 0.001000, 38.13s
2023-12-20 13:42:35,005 - INFO - Saved model at 22
2023-12-20 13:42:35,005 - INFO - Val loss decrease from 19.6991 to 19.4844, saving to ./libcity/cache/48492/model_cache/BGCN_PeMS04_epoch22.tar
2023-12-20 13:43:05,479 - INFO - epoch complete!
2023-12-20 13:43:05,479 - INFO - evaluating now!
2023-12-20 13:43:13,153 - INFO - Epoch [23/100] train_loss: 19.4481, val_loss: 20.2053, lr: 0.001000, 38.15s
2023-12-20 13:43:43,661 - INFO - epoch complete!
2023-12-20 13:43:43,661 - INFO - evaluating now!
2023-12-20 13:43:51,338 - INFO - Epoch [24/100] train_loss: 19.5751, val_loss: 19.6094, lr: 0.001000, 38.18s
2023-12-20 13:44:21,850 - INFO - epoch complete!
2023-12-20 13:44:21,850 - INFO - evaluating now!
2023-12-20 13:44:29,540 - INFO - Epoch [25/100] train_loss: 19.3961, val_loss: 19.9668, lr: 0.001000, 38.20s
2023-12-20 13:45:00,068 - INFO - epoch complete!
2023-12-20 13:45:00,068 - INFO - evaluating now!
2023-12-20 13:45:07,753 - INFO - Epoch [26/100] train_loss: 19.3981, val_loss: 19.9926, lr: 0.001000, 38.21s
2023-12-20 13:45:38,275 - INFO - epoch complete!
2023-12-20 13:45:38,275 - INFO - evaluating now!
2023-12-20 13:45:45,918 - INFO - Epoch [27/100] train_loss: 19.3664, val_loss: 19.8602, lr: 0.001000, 38.16s
2023-12-20 13:46:16,372 - INFO - epoch complete!
2023-12-20 13:46:16,372 - INFO - evaluating now!
2023-12-20 13:46:24,038 - INFO - Epoch [28/100] train_loss: 19.3065, val_loss: 19.7012, lr: 0.001000, 38.12s
2023-12-20 13:46:54,544 - INFO - epoch complete!
2023-12-20 13:46:54,545 - INFO - evaluating now!
2023-12-20 13:47:02,269 - INFO - Epoch [29/100] train_loss: 19.2162, val_loss: 19.7311, lr: 0.001000, 38.23s
2023-12-20 13:47:32,813 - INFO - epoch complete!
2023-12-20 13:47:32,813 - INFO - evaluating now!
2023-12-20 13:47:40,494 - INFO - Epoch [30/100] train_loss: 19.2055, val_loss: 19.3704, lr: 0.001000, 38.23s
2023-12-20 13:47:40,506 - INFO - Saved model at 30
2023-12-20 13:47:40,506 - INFO - Val loss decrease from 19.4844 to 19.3704, saving to ./libcity/cache/48492/model_cache/BGCN_PeMS04_epoch30.tar
2023-12-20 13:48:11,014 - INFO - epoch complete!
2023-12-20 13:48:11,014 - INFO - evaluating now!
2023-12-20 13:48:18,698 - INFO - Epoch [31/100] train_loss: 19.2174, val_loss: 19.5308, lr: 0.001000, 38.19s
2023-12-20 13:48:49,222 - INFO - epoch complete!
2023-12-20 13:48:49,222 - INFO - evaluating now!
2023-12-20 13:48:56,866 - INFO - Epoch [32/100] train_loss: 19.1955, val_loss: 19.7446, lr: 0.001000, 38.17s
2023-12-20 13:49:27,246 - INFO - epoch complete!
2023-12-20 13:49:27,246 - INFO - evaluating now!
2023-12-20 13:49:34,874 - INFO - Epoch [33/100] train_loss: 19.1623, val_loss: 19.5508, lr: 0.001000, 38.01s
2023-12-20 13:50:05,316 - INFO - epoch complete!
2023-12-20 13:50:05,316 - INFO - evaluating now!
2023-12-20 13:50:12,947 - INFO - Epoch [34/100] train_loss: 19.0642, val_loss: 19.3072, lr: 0.001000, 38.07s
2023-12-20 13:50:12,958 - INFO - Saved model at 34
2023-12-20 13:50:12,958 - INFO - Val loss decrease from 19.3704 to 19.3072, saving to ./libcity/cache/48492/model_cache/BGCN_PeMS04_epoch34.tar
2023-12-20 13:50:43,392 - INFO - epoch complete!
2023-12-20 13:50:43,392 - INFO - evaluating now!
2023-12-20 13:50:51,021 - INFO - Epoch [35/100] train_loss: 19.0522, val_loss: 19.5176, lr: 0.001000, 38.06s
2023-12-20 13:51:21,420 - INFO - epoch complete!
2023-12-20 13:51:21,420 - INFO - evaluating now!
2023-12-20 13:51:29,072 - INFO - Epoch [36/100] train_loss: 19.0724, val_loss: 19.4228, lr: 0.001000, 38.05s
2023-12-20 13:51:59,523 - INFO - epoch complete!
2023-12-20 13:51:59,524 - INFO - evaluating now!
2023-12-20 13:52:07,185 - INFO - Epoch [37/100] train_loss: 19.0175, val_loss: 19.3660, lr: 0.001000, 38.11s
2023-12-20 13:52:37,671 - INFO - epoch complete!
2023-12-20 13:52:37,672 - INFO - evaluating now!
2023-12-20 13:52:45,368 - INFO - Epoch [38/100] train_loss: 19.0687, val_loss: 19.4606, lr: 0.001000, 38.18s
2023-12-20 13:53:15,882 - INFO - epoch complete!
2023-12-20 13:53:15,883 - INFO - evaluating now!
2023-12-20 13:53:23,539 - INFO - Epoch [39/100] train_loss: 18.9966, val_loss: 19.3228, lr: 0.001000, 38.17s
2023-12-20 13:53:54,014 - INFO - epoch complete!
2023-12-20 13:53:54,014 - INFO - evaluating now!
2023-12-20 13:54:01,654 - INFO - Epoch [40/100] train_loss: 18.9229, val_loss: 19.5956, lr: 0.001000, 38.12s
2023-12-20 13:54:32,181 - INFO - epoch complete!
2023-12-20 13:54:32,181 - INFO - evaluating now!
2023-12-20 13:54:39,861 - INFO - Epoch [41/100] train_loss: 18.8910, val_loss: 19.2402, lr: 0.001000, 38.21s
2023-12-20 13:54:39,873 - INFO - Saved model at 41
2023-12-20 13:54:39,873 - INFO - Val loss decrease from 19.3072 to 19.2402, saving to ./libcity/cache/48492/model_cache/BGCN_PeMS04_epoch41.tar
2023-12-20 13:55:10,400 - INFO - epoch complete!
2023-12-20 13:55:10,401 - INFO - evaluating now!
2023-12-20 13:55:18,076 - INFO - Epoch [42/100] train_loss: 18.8834, val_loss: 19.1559, lr: 0.001000, 38.20s
2023-12-20 13:55:18,088 - INFO - Saved model at 42
2023-12-20 13:55:18,088 - INFO - Val loss decrease from 19.2402 to 19.1559, saving to ./libcity/cache/48492/model_cache/BGCN_PeMS04_epoch42.tar
2023-12-20 13:55:48,640 - INFO - epoch complete!
2023-12-20 13:55:48,640 - INFO - evaluating now!
2023-12-20 13:55:56,319 - INFO - Epoch [43/100] train_loss: 18.8396, val_loss: 19.0588, lr: 0.001000, 38.23s
2023-12-20 13:55:56,330 - INFO - Saved model at 43
2023-12-20 13:55:56,330 - INFO - Val loss decrease from 19.1559 to 19.0588, saving to ./libcity/cache/48492/model_cache/BGCN_PeMS04_epoch43.tar
2023-12-20 13:56:26,883 - INFO - epoch complete!
2023-12-20 13:56:26,883 - INFO - evaluating now!
2023-12-20 13:56:34,557 - INFO - Epoch [44/100] train_loss: 18.8105, val_loss: 19.3299, lr: 0.001000, 38.23s
2023-12-20 13:57:05,030 - INFO - epoch complete!
2023-12-20 13:57:05,030 - INFO - evaluating now!
2023-12-20 13:57:12,694 - INFO - Epoch [45/100] train_loss: 18.8054, val_loss: 19.3804, lr: 0.001000, 38.14s
2023-12-20 13:57:43,213 - INFO - epoch complete!
2023-12-20 13:57:43,213 - INFO - evaluating now!
2023-12-20 13:57:50,874 - INFO - Epoch [46/100] train_loss: 18.7721, val_loss: 19.2737, lr: 0.001000, 38.18s
2023-12-20 13:58:21,410 - INFO - epoch complete!
2023-12-20 13:58:21,410 - INFO - evaluating now!
2023-12-20 13:58:29,114 - INFO - Epoch [47/100] train_loss: 18.8086, val_loss: 19.2950, lr: 0.001000, 38.24s
2023-12-20 13:58:59,527 - INFO - epoch complete!
2023-12-20 13:58:59,527 - INFO - evaluating now!
2023-12-20 13:59:07,161 - INFO - Epoch [48/100] train_loss: 18.7232, val_loss: 19.1490, lr: 0.001000, 38.05s
2023-12-20 13:59:37,594 - INFO - epoch complete!
2023-12-20 13:59:37,594 - INFO - evaluating now!
2023-12-20 13:59:45,261 - INFO - Epoch [49/100] train_loss: 18.7416, val_loss: 19.0748, lr: 0.001000, 38.10s
2023-12-20 14:00:15,800 - INFO - epoch complete!
2023-12-20 14:00:15,800 - INFO - evaluating now!
2023-12-20 14:00:23,465 - INFO - Epoch [50/100] train_loss: 18.6801, val_loss: 19.5631, lr: 0.001000, 38.20s
2023-12-20 14:00:53,895 - INFO - epoch complete!
2023-12-20 14:00:53,895 - INFO - evaluating now!
2023-12-20 14:01:01,567 - INFO - Epoch [51/100] train_loss: 18.6706, val_loss: 19.0389, lr: 0.001000, 38.10s
2023-12-20 14:01:01,579 - INFO - Saved model at 51
2023-12-20 14:01:01,579 - INFO - Val loss decrease from 19.0588 to 19.0389, saving to ./libcity/cache/48492/model_cache/BGCN_PeMS04_epoch51.tar
2023-12-20 14:01:32,021 - INFO - epoch complete!
2023-12-20 14:01:32,021 - INFO - evaluating now!
2023-12-20 14:01:39,658 - INFO - Epoch [52/100] train_loss: 18.6444, val_loss: 19.0303, lr: 0.001000, 38.08s
2023-12-20 14:01:39,669 - INFO - Saved model at 52
2023-12-20 14:01:39,669 - INFO - Val loss decrease from 19.0389 to 19.0303, saving to ./libcity/cache/48492/model_cache/BGCN_PeMS04_epoch52.tar
2023-12-20 14:02:10,106 - INFO - epoch complete!
2023-12-20 14:02:10,106 - INFO - evaluating now!
2023-12-20 14:02:17,752 - INFO - Epoch [53/100] train_loss: 18.6196, val_loss: 19.1568, lr: 0.001000, 38.08s
2023-12-20 14:02:48,166 - INFO - epoch complete!
2023-12-20 14:02:48,166 - INFO - evaluating now!
2023-12-20 14:02:55,811 - INFO - Epoch [54/100] train_loss: 18.6365, val_loss: 19.0147, lr: 0.001000, 38.06s
2023-12-20 14:02:55,822 - INFO - Saved model at 54
2023-12-20 14:02:55,822 - INFO - Val loss decrease from 19.0303 to 19.0147, saving to ./libcity/cache/48492/model_cache/BGCN_PeMS04_epoch54.tar
2023-12-20 14:03:26,204 - INFO - epoch complete!
2023-12-20 14:03:26,204 - INFO - evaluating now!
2023-12-20 14:03:33,886 - INFO - Epoch [55/100] train_loss: 18.6691, val_loss: 18.9541, lr: 0.001000, 38.06s
2023-12-20 14:03:33,896 - INFO - Saved model at 55
2023-12-20 14:03:33,897 - INFO - Val loss decrease from 19.0147 to 18.9541, saving to ./libcity/cache/48492/model_cache/BGCN_PeMS04_epoch55.tar
2023-12-20 14:04:04,358 - INFO - epoch complete!
2023-12-20 14:04:04,359 - INFO - evaluating now!
2023-12-20 14:04:12,029 - INFO - Epoch [56/100] train_loss: 18.6198, val_loss: 18.8556, lr: 0.001000, 38.13s
2023-12-20 14:04:12,040 - INFO - Saved model at 56
2023-12-20 14:04:12,040 - INFO - Val loss decrease from 18.9541 to 18.8556, saving to ./libcity/cache/48492/model_cache/BGCN_PeMS04_epoch56.tar
2023-12-20 14:04:42,520 - INFO - epoch complete!
2023-12-20 14:04:42,520 - INFO - evaluating now!
2023-12-20 14:04:50,233 - INFO - Epoch [57/100] train_loss: 18.5576, val_loss: 19.1184, lr: 0.001000, 38.19s
2023-12-20 14:05:20,733 - INFO - epoch complete!
2023-12-20 14:05:20,733 - INFO - evaluating now!
2023-12-20 14:05:28,404 - INFO - Epoch [58/100] train_loss: 18.5589, val_loss: 18.8978, lr: 0.001000, 38.17s
2023-12-20 14:05:58,911 - INFO - epoch complete!
2023-12-20 14:05:58,911 - INFO - evaluating now!
2023-12-20 14:06:06,588 - INFO - Epoch [59/100] train_loss: 18.5349, val_loss: 18.8861, lr: 0.001000, 38.18s
2023-12-20 14:06:37,116 - INFO - epoch complete!
2023-12-20 14:06:37,117 - INFO - evaluating now!
2023-12-20 14:06:44,799 - INFO - Epoch [60/100] train_loss: 18.5829, val_loss: 19.1741, lr: 0.001000, 38.21s
2023-12-20 14:07:15,303 - INFO - epoch complete!
2023-12-20 14:07:15,303 - INFO - evaluating now!
2023-12-20 14:07:22,989 - INFO - Epoch [61/100] train_loss: 18.4667, val_loss: 19.1497, lr: 0.001000, 38.19s
2023-12-20 14:07:53,465 - INFO - epoch complete!
2023-12-20 14:07:53,465 - INFO - evaluating now!
2023-12-20 14:08:01,135 - INFO - Epoch [62/100] train_loss: 18.4595, val_loss: 18.9218, lr: 0.001000, 38.15s
2023-12-20 14:08:31,711 - INFO - epoch complete!
2023-12-20 14:08:31,711 - INFO - evaluating now!
2023-12-20 14:08:39,382 - INFO - Epoch [63/100] train_loss: 18.4573, val_loss: 18.7539, lr: 0.001000, 38.25s
2023-12-20 14:08:39,393 - INFO - Saved model at 63
2023-12-20 14:08:39,393 - INFO - Val loss decrease from 18.8556 to 18.7539, saving to ./libcity/cache/48492/model_cache/BGCN_PeMS04_epoch63.tar
2023-12-20 14:09:09,928 - INFO - epoch complete!
2023-12-20 14:09:09,928 - INFO - evaluating now!
2023-12-20 14:09:17,620 - INFO - Epoch [64/100] train_loss: 18.4560, val_loss: 18.7926, lr: 0.001000, 38.23s
2023-12-20 14:09:48,179 - INFO - epoch complete!
2023-12-20 14:09:48,179 - INFO - evaluating now!
2023-12-20 14:09:55,856 - INFO - Epoch [65/100] train_loss: 18.4195, val_loss: 19.1024, lr: 0.001000, 38.24s
2023-12-20 14:10:26,408 - INFO - epoch complete!
2023-12-20 14:10:26,408 - INFO - evaluating now!
2023-12-20 14:10:34,089 - INFO - Epoch [66/100] train_loss: 18.4037, val_loss: 18.9829, lr: 0.001000, 38.23s
2023-12-20 14:11:04,582 - INFO - epoch complete!
2023-12-20 14:11:04,583 - INFO - evaluating now!
2023-12-20 14:11:12,228 - INFO - Epoch [67/100] train_loss: 18.4088, val_loss: 18.9183, lr: 0.001000, 38.14s
2023-12-20 14:11:42,679 - INFO - epoch complete!
2023-12-20 14:11:42,679 - INFO - evaluating now!
2023-12-20 14:11:50,354 - INFO - Epoch [68/100] train_loss: 18.3804, val_loss: 18.7971, lr: 0.001000, 38.13s
2023-12-20 14:12:20,857 - INFO - epoch complete!
2023-12-20 14:12:20,857 - INFO - evaluating now!
2023-12-20 14:12:28,520 - INFO - Epoch [69/100] train_loss: 18.3741, val_loss: 19.0046, lr: 0.001000, 38.17s
2023-12-20 14:12:58,991 - INFO - epoch complete!
2023-12-20 14:12:58,991 - INFO - evaluating now!
2023-12-20 14:13:06,653 - INFO - Epoch [70/100] train_loss: 18.3394, val_loss: 19.0933, lr: 0.001000, 38.13s
2023-12-20 14:13:37,106 - INFO - epoch complete!
2023-12-20 14:13:37,106 - INFO - evaluating now!
2023-12-20 14:13:44,766 - INFO - Epoch [71/100] train_loss: 18.3035, val_loss: 18.9226, lr: 0.001000, 38.11s
2023-12-20 14:14:15,268 - INFO - epoch complete!
2023-12-20 14:14:15,268 - INFO - evaluating now!
2023-12-20 14:14:22,940 - INFO - Epoch [72/100] train_loss: 18.3417, val_loss: 18.7456, lr: 0.001000, 38.17s
2023-12-20 14:14:22,951 - INFO - Saved model at 72
2023-12-20 14:14:22,951 - INFO - Val loss decrease from 18.7539 to 18.7456, saving to ./libcity/cache/48492/model_cache/BGCN_PeMS04_epoch72.tar
2023-12-20 14:14:53,387 - INFO - epoch complete!
2023-12-20 14:14:53,387 - INFO - evaluating now!
2023-12-20 14:15:01,071 - INFO - Epoch [73/100] train_loss: 18.3158, val_loss: 18.7833, lr: 0.001000, 38.12s
2023-12-20 14:15:31,521 - INFO - epoch complete!
2023-12-20 14:15:31,521 - INFO - evaluating now!
2023-12-20 14:15:39,206 - INFO - Epoch [74/100] train_loss: 18.3205, val_loss: 18.8655, lr: 0.001000, 38.13s
2023-12-20 14:16:09,683 - INFO - epoch complete!
2023-12-20 14:16:09,683 - INFO - evaluating now!
2023-12-20 14:16:17,344 - INFO - Epoch [75/100] train_loss: 18.2942, val_loss: 19.2208, lr: 0.001000, 38.14s
2023-12-20 14:16:47,854 - INFO - epoch complete!
2023-12-20 14:16:47,854 - INFO - evaluating now!
2023-12-20 14:16:55,530 - INFO - Epoch [76/100] train_loss: 18.2888, val_loss: 18.8430, lr: 0.001000, 38.19s
2023-12-20 14:17:25,942 - INFO - epoch complete!
2023-12-20 14:17:25,942 - INFO - evaluating now!
2023-12-20 14:17:33,575 - INFO - Epoch [77/100] train_loss: 18.2730, val_loss: 18.9123, lr: 0.001000, 38.04s
2023-12-20 14:18:04,029 - INFO - epoch complete!
2023-12-20 14:18:04,029 - INFO - evaluating now!
2023-12-20 14:18:11,670 - INFO - Epoch [78/100] train_loss: 18.2510, val_loss: 19.1059, lr: 0.001000, 38.10s
2023-12-20 14:18:42,129 - INFO - epoch complete!
2023-12-20 14:18:42,129 - INFO - evaluating now!
2023-12-20 14:18:49,760 - INFO - Epoch [79/100] train_loss: 18.2609, val_loss: 18.7130, lr: 0.001000, 38.09s
2023-12-20 14:18:49,770 - INFO - Saved model at 79
2023-12-20 14:18:49,771 - INFO - Val loss decrease from 18.7456 to 18.7130, saving to ./libcity/cache/48492/model_cache/BGCN_PeMS04_epoch79.tar
2023-12-20 14:19:20,240 - INFO - epoch complete!
2023-12-20 14:19:20,240 - INFO - evaluating now!
2023-12-20 14:19:27,903 - INFO - Epoch [80/100] train_loss: 18.2553, val_loss: 18.9096, lr: 0.001000, 38.13s
2023-12-20 14:19:58,378 - INFO - epoch complete!
2023-12-20 14:19:58,378 - INFO - evaluating now!
2023-12-20 14:20:06,012 - INFO - Epoch [81/100] train_loss: 18.2466, val_loss: 18.7791, lr: 0.001000, 38.11s
2023-12-20 14:20:36,481 - INFO - epoch complete!
2023-12-20 14:20:36,481 - INFO - evaluating now!
2023-12-20 14:20:44,157 - INFO - Epoch [82/100] train_loss: 18.2052, val_loss: 18.9215, lr: 0.001000, 38.14s
2023-12-20 14:21:14,629 - INFO - epoch complete!
2023-12-20 14:21:14,629 - INFO - evaluating now!
2023-12-20 14:21:22,295 - INFO - Epoch [83/100] train_loss: 18.2076, val_loss: 18.9557, lr: 0.001000, 38.14s
2023-12-20 14:21:52,800 - INFO - epoch complete!
2023-12-20 14:21:52,800 - INFO - evaluating now!
2023-12-20 14:22:00,495 - INFO - Epoch [84/100] train_loss: 18.1745, val_loss: 18.8895, lr: 0.001000, 38.20s
2023-12-20 14:22:30,990 - INFO - epoch complete!
2023-12-20 14:22:30,990 - INFO - evaluating now!
2023-12-20 14:22:38,661 - INFO - Epoch [85/100] train_loss: 18.1917, val_loss: 18.7771, lr: 0.001000, 38.17s
2023-12-20 14:23:09,153 - INFO - epoch complete!
2023-12-20 14:23:09,154 - INFO - evaluating now!
2023-12-20 14:23:16,841 - INFO - Epoch [86/100] train_loss: 18.1579, val_loss: 18.8035, lr: 0.001000, 38.18s
2023-12-20 14:23:47,395 - INFO - epoch complete!
2023-12-20 14:23:47,395 - INFO - evaluating now!
2023-12-20 14:23:55,070 - INFO - Epoch [87/100] train_loss: 18.1856, val_loss: 18.8837, lr: 0.001000, 38.23s
2023-12-20 14:24:25,591 - INFO - epoch complete!
2023-12-20 14:24:25,591 - INFO - evaluating now!
2023-12-20 14:24:33,268 - INFO - Epoch [88/100] train_loss: 18.1453, val_loss: 18.8110, lr: 0.001000, 38.20s
2023-12-20 14:25:03,839 - INFO - epoch complete!
2023-12-20 14:25:03,839 - INFO - evaluating now!
2023-12-20 14:25:11,506 - INFO - Epoch [89/100] train_loss: 18.1088, val_loss: 18.7780, lr: 0.001000, 38.24s
2023-12-20 14:25:42,052 - INFO - epoch complete!
2023-12-20 14:25:42,052 - INFO - evaluating now!
2023-12-20 14:25:49,762 - INFO - Epoch [90/100] train_loss: 18.1167, val_loss: 18.8369, lr: 0.001000, 38.26s
2023-12-20 14:26:20,361 - INFO - epoch complete!
2023-12-20 14:26:20,361 - INFO - evaluating now!
2023-12-20 14:26:28,055 - INFO - Epoch [91/100] train_loss: 18.1312, val_loss: 18.7862, lr: 0.001000, 38.29s
2023-12-20 14:26:58,519 - INFO - epoch complete!
2023-12-20 14:26:58,519 - INFO - evaluating now!
2023-12-20 14:27:06,156 - INFO - Epoch [92/100] train_loss: 18.1150, val_loss: 18.6392, lr: 0.001000, 38.10s
2023-12-20 14:27:06,167 - INFO - Saved model at 92
2023-12-20 14:27:06,167 - INFO - Val loss decrease from 18.7130 to 18.6392, saving to ./libcity/cache/48492/model_cache/BGCN_PeMS04_epoch92.tar
2023-12-20 14:27:36,555 - INFO - epoch complete!
2023-12-20 14:27:36,555 - INFO - evaluating now!
2023-12-20 14:27:44,213 - INFO - Epoch [93/100] train_loss: 18.1066, val_loss: 18.7289, lr: 0.001000, 38.05s
2023-12-20 14:28:14,704 - INFO - epoch complete!
2023-12-20 14:28:14,704 - INFO - evaluating now!
2023-12-20 14:28:22,391 - INFO - Epoch [94/100] train_loss: 18.0499, val_loss: 18.7134, lr: 0.001000, 38.18s
2023-12-20 14:28:52,800 - INFO - epoch complete!
2023-12-20 14:28:52,800 - INFO - evaluating now!
2023-12-20 14:29:00,434 - INFO - Epoch [95/100] train_loss: 18.0889, val_loss: 18.7924, lr: 0.001000, 38.04s
2023-12-20 14:29:30,984 - INFO - epoch complete!
2023-12-20 14:29:30,984 - INFO - evaluating now!
2023-12-20 14:29:38,674 - INFO - Epoch [96/100] train_loss: 18.0620, val_loss: 18.6899, lr: 0.001000, 38.24s
2023-12-20 14:30:09,241 - INFO - epoch complete!
2023-12-20 14:30:09,241 - INFO - evaluating now!
2023-12-20 14:30:16,920 - INFO - Epoch [97/100] train_loss: 18.0617, val_loss: 18.5539, lr: 0.001000, 38.25s
2023-12-20 14:30:16,931 - INFO - Saved model at 97
2023-12-20 14:30:16,931 - INFO - Val loss decrease from 18.6392 to 18.5539, saving to ./libcity/cache/48492/model_cache/BGCN_PeMS04_epoch97.tar
2023-12-20 14:30:47,485 - INFO - epoch complete!
2023-12-20 14:30:47,485 - INFO - evaluating now!
2023-12-20 14:30:55,144 - INFO - Epoch [98/100] train_loss: 18.0548, val_loss: 18.9954, lr: 0.001000, 38.21s
2023-12-20 14:31:25,579 - INFO - epoch complete!
2023-12-20 14:31:25,580 - INFO - evaluating now!
2023-12-20 14:31:33,207 - INFO - Epoch [99/100] train_loss: 18.0933, val_loss: 18.7244, lr: 0.001000, 38.06s
2023-12-20 14:31:33,207 - INFO - Trained totally 100 epochs, average train time is 30.633s, average eval time is 7.711s
2023-12-20 14:31:33,218 - INFO - Loaded model at 97
2023-12-20 14:31:33,218 - INFO - Saved model at ./libcity/cache/48492/model_cache/BGCN_PeMS04.m
2023-12-20 14:31:33,229 - INFO - Start evaluating ...
2023-12-20 14:31:42,716 - INFO - Note that you select the single mode to evaluate!
2023-12-20 14:31:42,717 - INFO - Evaluate result is saved at ./libcity/cache/48492/evaluate_cache/2023_12_20_14_31_42_BGCN_PeMS04.csv
2023-12-20 14:31:42,721 - INFO - 
          MAE  MAPE          MSE       RMSE  masked_MAE  masked_MAPE   masked_MSE  masked_RMSE        R2      EVAR
1   16.765079   inf   764.906555  27.656944   16.636797     0.119388   717.859314    26.792896  0.969274  0.969352
2   17.415798   inf   850.285278  29.159651   17.222364     0.124972   775.444946    27.846811  0.965839  0.965934
3   17.913940   inf   906.847656  30.113911   17.696110     0.127307   819.997803    28.635603  0.963570  0.963641
4   18.298872   inf   957.122925  30.937403   18.062370     0.130685   859.795349    29.322268  0.961547  0.961659
5   18.627417   inf   989.308899  31.453281   18.390684     0.133627   892.433350    29.873623  0.960269  0.960402
6   18.868244   inf  1015.176270  31.861832   18.639681     0.133444   921.440735    30.355242  0.959233  0.959338
7   19.135145   inf  1049.916260  32.402412   18.895922     0.135714   949.058594    30.806795  0.957859  0.957986
8   19.338427   inf  1069.478149  32.702877   19.107851     0.136980   972.776855    31.189371  0.957072  0.957140
9   19.561621   inf  1096.201294  33.108932   19.325445     0.138651   997.720825    31.586720  0.955996  0.956081
10  19.746370   inf  1120.161743  33.468819   19.497580     0.142092  1013.913757    31.842012  0.955063  0.955168
11  20.003565   inf  1142.482666  33.800632   19.745031     0.146684  1032.238403    32.128468  0.954191  0.954272
12  20.242479   inf  1163.383301  34.108406   19.979597     0.146227  1052.702271    32.445374  0.953385  0.953453
