2023-12-20 14:31:59,161 - INFO - Log directory: ./libcity/log
2023-12-20 14:31:59,170 - INFO - Begin pipeline, task=traffic_state_pred, model_name=GWNET, dataset_name=PeMS04, exp_id=33958
2023-12-20 14:31:59,170 - INFO - {'task': 'traffic_state_pred', 'model': 'GWNET', 'dataset': 'PeMS04', 'saved_model': True, 'train': True, 'seed': 0, 'dataset_class': 'TrafficStatePointDataset', 'executor': 'TrafficStateExecutor', 'evaluator': 'TrafficStateEvaluator', 'dropout': 0.3, 'blocks': 4, 'layers': 2, 'apt_layer': True, 'gcn_bool': True, 'addaptadj': True, 'adjtype': 'doubletransition', 'bidir_adj_mx': False, 'randomadj': True, 'aptonly': True, 'kernel_size': 2, 'nhid': 32, 'residual_channels': 32, 'dilation_channels': 32, 'skip_channels': 256, 'end_channels': 512, 'scaler': 'standard', 'load_external': False, 'normal_external': False, 'ext_scaler': 'none', 'add_time_in_day': False, 'add_day_in_week': False, 'max_epoch': 100, 'learner': 'adam', 'learning_rate': 0.001, 'lr_decay': False, 'clip_grad_norm': True, 'max_grad_norm': 5, 'use_early_stop': False, 'batch_size': 64, 'cache_dataset': True, 'num_workers': 0, 'pad_with_last_sample': True, 'train_rate': 0.6, 'eval_rate': 0.2, 'input_window': 12, 'output_window': 12, 'gpu': True, 'gpu_id': 0, 'train_loss': 'none', 'epoch': 0, 'weight_decay': 0, 'lr_epsilon': 1e-08, 'lr_beta1': 0.9, 'lr_beta2': 0.999, 'lr_alpha': 0.99, 'lr_momentum': 0, 'lr_scheduler': 'multisteplr', 'lr_decay_ratio': 0.1, 'steps': [5, 20, 40, 70], 'step_size': 10, 'lr_T_max': 30, 'lr_eta_min': 0, 'lr_patience': 10, 'lr_threshold': 0.0001, 'patience': 50, 'log_level': 'INFO', 'log_every': 1, 'load_best_epoch': True, 'hyper_tune': False, 'metrics': ['MAE', 'MAPE', 'MSE', 'RMSE', 'masked_MAE', 'masked_MAPE', 'masked_MSE', 'masked_RMSE', 'R2', 'EVAR'], 'evaluator_mode': 'single', 'save_mode': ['csv'], 'geo': {'including_types': ['Point'], 'Point': {}}, 'rel': {'including_types': ['geo'], 'geo': {'cost': 'num'}}, 'dyna': {'including_types': ['state'], 'state': {'entity_id': 'geo_id', 'traffic_flow': 'num', 'traffic_occupancy': 'num', 'traffic_speed': 'num'}}, 'data_col': ['traffic_flow'], 'weight_col': 'cost', 'data_files': ['PeMS04'], 'geo_file': 'PeMS04', 'rel_file': 'PeMS04', 'output_dim': 1, 'time_intervals': 300, 'init_weight_inf_or_zero': 'zero', 'set_weight_link_or_dist': 'link', 'calculate_weight_adj': False, 'weight_adj_epsilon': 0, 'device': device(type='cuda', index=0), 'exp_id': 33958}
2023-12-20 14:31:59,172 - INFO - Loaded file PeMS04.geo, num_nodes=307
2023-12-20 14:31:59,173 - INFO - set_weight_link_or_dist: link
2023-12-20 14:31:59,173 - INFO - init_weight_inf_or_zero: zero
2023-12-20 14:31:59,174 - INFO - Loaded file PeMS04.rel, shape=(307, 307)
2023-12-20 14:31:59,174 - INFO - Loading ./libcity/cache/dataset_cache/point_based_PeMS04_12_12_0.6_0.2_standard_64_False_False_False_True.npz
2023-12-20 14:31:59,754 - INFO - train	x: (10181, 12, 307, 1), y: (10181, 12, 307, 1)
2023-12-20 14:31:59,754 - INFO - eval	x: (3394, 12, 307, 1), y: (3394, 12, 307, 1)
2023-12-20 14:31:59,754 - INFO - test	x: (3394, 12, 307, 1), y: (3394, 12, 307, 1)
2023-12-20 14:31:59,843 - INFO - StandardScaler mean: 207.22733840505313, std: 156.47765518492758
2023-12-20 14:31:59,843 - INFO - NoneScaler
2023-12-20 14:32:00,402 - INFO - receptive_field: 13
2023-12-20 14:32:00,516 - INFO - GWNET(
  (filter_convs): ModuleList(
    (0): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1))
    (1): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1), dilation=(2, 2))
    (2): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1))
    (3): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1), dilation=(2, 2))
    (4): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1))
    (5): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1), dilation=(2, 2))
    (6): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1))
    (7): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1), dilation=(2, 2))
  )
  (gate_convs): ModuleList(
    (0): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1))
    (1): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1), dilation=(2, 2))
    (2): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1))
    (3): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1), dilation=(2, 2))
    (4): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1))
    (5): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1), dilation=(2, 2))
    (6): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1))
    (7): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 1), dilation=(2, 2))
  )
  (residual_convs): ModuleList(
    (0-7): 8 x Conv1d(32, 32, kernel_size=(1, 1), stride=(1,))
  )
  (skip_convs): ModuleList(
    (0-7): 8 x Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1))
  )
  (bn): ModuleList(
    (0-7): 8 x BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (gconv): ModuleList(
    (0-7): 8 x GCN(
      (nconv): NConv()
      (mlp): Linear(
        (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
  (start_conv): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1))
  (end_conv_1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
  (end_conv_2): Conv2d(512, 12, kernel_size=(1, 1), stride=(1, 1))
)
2023-12-20 14:32:00,516 - INFO - nodevec1	torch.Size([307, 10])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - nodevec2	torch.Size([10, 307])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - filter_convs.0.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - filter_convs.0.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - filter_convs.1.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - filter_convs.1.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - filter_convs.2.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - filter_convs.2.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - filter_convs.3.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - filter_convs.3.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - filter_convs.4.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - filter_convs.4.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - filter_convs.5.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - filter_convs.5.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - filter_convs.6.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - filter_convs.6.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - filter_convs.7.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - filter_convs.7.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - gate_convs.0.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - gate_convs.0.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - gate_convs.1.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - gate_convs.1.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - gate_convs.2.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - gate_convs.2.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - gate_convs.3.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - gate_convs.3.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - gate_convs.4.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - gate_convs.4.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - gate_convs.5.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - gate_convs.5.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - gate_convs.6.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - gate_convs.6.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - gate_convs.7.weight	torch.Size([32, 32, 1, 2])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - gate_convs.7.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - residual_convs.0.weight	torch.Size([32, 32, 1, 1])	cuda:0	True
2023-12-20 14:32:00,516 - INFO - residual_convs.0.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - residual_convs.1.weight	torch.Size([32, 32, 1, 1])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - residual_convs.1.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - residual_convs.2.weight	torch.Size([32, 32, 1, 1])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - residual_convs.2.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - residual_convs.3.weight	torch.Size([32, 32, 1, 1])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - residual_convs.3.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - residual_convs.4.weight	torch.Size([32, 32, 1, 1])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - residual_convs.4.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - residual_convs.5.weight	torch.Size([32, 32, 1, 1])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - residual_convs.5.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - residual_convs.6.weight	torch.Size([32, 32, 1, 1])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - residual_convs.6.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - residual_convs.7.weight	torch.Size([32, 32, 1, 1])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - residual_convs.7.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - skip_convs.0.weight	torch.Size([256, 32, 1, 1])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - skip_convs.0.bias	torch.Size([256])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - skip_convs.1.weight	torch.Size([256, 32, 1, 1])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - skip_convs.1.bias	torch.Size([256])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - skip_convs.2.weight	torch.Size([256, 32, 1, 1])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - skip_convs.2.bias	torch.Size([256])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - skip_convs.3.weight	torch.Size([256, 32, 1, 1])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - skip_convs.3.bias	torch.Size([256])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - skip_convs.4.weight	torch.Size([256, 32, 1, 1])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - skip_convs.4.bias	torch.Size([256])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - skip_convs.5.weight	torch.Size([256, 32, 1, 1])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - skip_convs.5.bias	torch.Size([256])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - skip_convs.6.weight	torch.Size([256, 32, 1, 1])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - skip_convs.6.bias	torch.Size([256])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - skip_convs.7.weight	torch.Size([256, 32, 1, 1])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - skip_convs.7.bias	torch.Size([256])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - bn.0.weight	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - bn.0.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - bn.1.weight	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - bn.1.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - bn.2.weight	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - bn.2.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - bn.3.weight	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - bn.3.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - bn.4.weight	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - bn.4.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - bn.5.weight	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - bn.5.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - bn.6.weight	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - bn.6.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - bn.7.weight	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - bn.7.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - gconv.0.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - gconv.0.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - gconv.1.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - gconv.1.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - gconv.2.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - gconv.2.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - gconv.3.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - gconv.3.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - gconv.4.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - gconv.4.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - gconv.5.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - gconv.5.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - gconv.6.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - gconv.6.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - gconv.7.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - gconv.7.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - start_conv.weight	torch.Size([32, 1, 1, 1])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - start_conv.bias	torch.Size([32])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - end_conv_1.weight	torch.Size([512, 256, 1, 1])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - end_conv_1.bias	torch.Size([512])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - end_conv_2.weight	torch.Size([12, 512, 1, 1])	cuda:0	True
2023-12-20 14:32:00,517 - INFO - end_conv_2.bias	torch.Size([12])	cuda:0	True
2023-12-20 14:32:00,518 - INFO - Total parameter numbers: 278600
2023-12-20 14:32:00,518 - INFO - You select `adam` optimizer.
2023-12-20 14:32:00,518 - WARNING - Received none train loss func and will use the loss func defined in the model.
2023-12-20 14:32:00,518 - INFO - Start training ...
2023-12-20 14:32:00,518 - INFO - num_batches:160
2023-12-20 14:32:32,176 - INFO - epoch complete!
2023-12-20 14:32:32,176 - INFO - evaluating now!
2023-12-20 14:32:39,968 - INFO - Epoch [0/100] train_loss: 36.3620, val_loss: 30.5439, lr: 0.001000, 39.45s
2023-12-20 14:32:39,980 - INFO - Saved model at 0
2023-12-20 14:32:39,980 - INFO - Val loss decrease from inf to 30.5439, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch0.tar
2023-12-20 14:33:11,247 - INFO - epoch complete!
2023-12-20 14:33:11,248 - INFO - evaluating now!
2023-12-20 14:33:19,006 - INFO - Epoch [1/100] train_loss: 28.0039, val_loss: 29.7142, lr: 0.001000, 39.03s
2023-12-20 14:33:19,016 - INFO - Saved model at 1
2023-12-20 14:33:19,016 - INFO - Val loss decrease from 30.5439 to 29.7142, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch1.tar
2023-12-20 14:33:50,269 - INFO - epoch complete!
2023-12-20 14:33:50,269 - INFO - evaluating now!
2023-12-20 14:33:58,034 - INFO - Epoch [2/100] train_loss: 26.1918, val_loss: 27.0464, lr: 0.001000, 39.02s
2023-12-20 14:33:58,045 - INFO - Saved model at 2
2023-12-20 14:33:58,045 - INFO - Val loss decrease from 29.7142 to 27.0464, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch2.tar
2023-12-20 14:34:29,326 - INFO - epoch complete!
2023-12-20 14:34:29,326 - INFO - evaluating now!
2023-12-20 14:34:37,080 - INFO - Epoch [3/100] train_loss: 25.1548, val_loss: 26.5650, lr: 0.001000, 39.03s
2023-12-20 14:34:37,090 - INFO - Saved model at 3
2023-12-20 14:34:37,090 - INFO - Val loss decrease from 27.0464 to 26.5650, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch3.tar
2023-12-20 14:35:08,362 - INFO - epoch complete!
2023-12-20 14:35:08,362 - INFO - evaluating now!
2023-12-20 14:35:16,140 - INFO - Epoch [4/100] train_loss: 24.5376, val_loss: 24.3015, lr: 0.001000, 39.05s
2023-12-20 14:35:16,151 - INFO - Saved model at 4
2023-12-20 14:35:16,151 - INFO - Val loss decrease from 26.5650 to 24.3015, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch4.tar
2023-12-20 14:35:47,490 - INFO - epoch complete!
2023-12-20 14:35:47,490 - INFO - evaluating now!
2023-12-20 14:35:55,284 - INFO - Epoch [5/100] train_loss: 24.1393, val_loss: 23.9044, lr: 0.001000, 39.13s
2023-12-20 14:35:55,294 - INFO - Saved model at 5
2023-12-20 14:35:55,294 - INFO - Val loss decrease from 24.3015 to 23.9044, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch5.tar
2023-12-20 14:36:26,570 - INFO - epoch complete!
2023-12-20 14:36:26,570 - INFO - evaluating now!
2023-12-20 14:36:34,377 - INFO - Epoch [6/100] train_loss: 23.7158, val_loss: 25.8495, lr: 0.001000, 39.08s
2023-12-20 14:37:05,771 - INFO - epoch complete!
2023-12-20 14:37:05,771 - INFO - evaluating now!
2023-12-20 14:37:13,595 - INFO - Epoch [7/100] train_loss: 23.5224, val_loss: 25.9950, lr: 0.001000, 39.22s
2023-12-20 14:37:45,010 - INFO - epoch complete!
2023-12-20 14:37:45,010 - INFO - evaluating now!
2023-12-20 14:37:52,825 - INFO - Epoch [8/100] train_loss: 23.2378, val_loss: 26.6583, lr: 0.001000, 39.23s
2023-12-20 14:38:24,199 - INFO - epoch complete!
2023-12-20 14:38:24,199 - INFO - evaluating now!
2023-12-20 14:38:32,010 - INFO - Epoch [9/100] train_loss: 22.9269, val_loss: 22.9788, lr: 0.001000, 39.18s
2023-12-20 14:38:32,021 - INFO - Saved model at 9
2023-12-20 14:38:32,021 - INFO - Val loss decrease from 23.9044 to 22.9788, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch9.tar
2023-12-20 14:39:03,375 - INFO - epoch complete!
2023-12-20 14:39:03,375 - INFO - evaluating now!
2023-12-20 14:39:11,150 - INFO - Epoch [10/100] train_loss: 22.8947, val_loss: 23.6559, lr: 0.001000, 39.13s
2023-12-20 14:39:42,488 - INFO - epoch complete!
2023-12-20 14:39:42,489 - INFO - evaluating now!
2023-12-20 14:39:50,285 - INFO - Epoch [11/100] train_loss: 22.6872, val_loss: 24.3150, lr: 0.001000, 39.13s
2023-12-20 14:40:21,592 - INFO - epoch complete!
2023-12-20 14:40:21,592 - INFO - evaluating now!
2023-12-20 14:40:29,371 - INFO - Epoch [12/100] train_loss: 22.5202, val_loss: 23.0856, lr: 0.001000, 39.09s
2023-12-20 14:41:00,694 - INFO - epoch complete!
2023-12-20 14:41:00,694 - INFO - evaluating now!
2023-12-20 14:41:08,502 - INFO - Epoch [13/100] train_loss: 22.3438, val_loss: 22.8180, lr: 0.001000, 39.13s
2023-12-20 14:41:08,513 - INFO - Saved model at 13
2023-12-20 14:41:08,513 - INFO - Val loss decrease from 22.9788 to 22.8180, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch13.tar
2023-12-20 14:41:39,811 - INFO - epoch complete!
2023-12-20 14:41:39,811 - INFO - evaluating now!
2023-12-20 14:41:47,592 - INFO - Epoch [14/100] train_loss: 22.1299, val_loss: 23.4932, lr: 0.001000, 39.08s
2023-12-20 14:42:18,865 - INFO - epoch complete!
2023-12-20 14:42:18,866 - INFO - evaluating now!
2023-12-20 14:42:26,624 - INFO - Epoch [15/100] train_loss: 22.1929, val_loss: 23.5284, lr: 0.001000, 39.03s
2023-12-20 14:42:57,862 - INFO - epoch complete!
2023-12-20 14:42:57,862 - INFO - evaluating now!
2023-12-20 14:43:05,638 - INFO - Epoch [16/100] train_loss: 21.9179, val_loss: 23.3969, lr: 0.001000, 39.01s
2023-12-20 14:43:36,869 - INFO - epoch complete!
2023-12-20 14:43:36,869 - INFO - evaluating now!
2023-12-20 14:43:44,651 - INFO - Epoch [17/100] train_loss: 21.8660, val_loss: 22.5577, lr: 0.001000, 39.01s
2023-12-20 14:43:44,662 - INFO - Saved model at 17
2023-12-20 14:43:44,662 - INFO - Val loss decrease from 22.8180 to 22.5577, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch17.tar
2023-12-20 14:44:15,904 - INFO - epoch complete!
2023-12-20 14:44:15,905 - INFO - evaluating now!
2023-12-20 14:44:23,674 - INFO - Epoch [18/100] train_loss: 21.7861, val_loss: 22.0550, lr: 0.001000, 39.01s
2023-12-20 14:44:23,693 - INFO - Saved model at 18
2023-12-20 14:44:23,693 - INFO - Val loss decrease from 22.5577 to 22.0550, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch18.tar
2023-12-20 14:44:54,942 - INFO - epoch complete!
2023-12-20 14:44:54,942 - INFO - evaluating now!
2023-12-20 14:45:02,715 - INFO - Epoch [19/100] train_loss: 21.5724, val_loss: 22.6957, lr: 0.001000, 39.02s
2023-12-20 14:45:33,914 - INFO - epoch complete!
2023-12-20 14:45:33,914 - INFO - evaluating now!
2023-12-20 14:45:41,677 - INFO - Epoch [20/100] train_loss: 21.4974, val_loss: 22.3532, lr: 0.001000, 38.96s
2023-12-20 14:46:12,916 - INFO - epoch complete!
2023-12-20 14:46:12,916 - INFO - evaluating now!
2023-12-20 14:46:20,713 - INFO - Epoch [21/100] train_loss: 21.2755, val_loss: 22.3354, lr: 0.001000, 39.04s
2023-12-20 14:46:52,010 - INFO - epoch complete!
2023-12-20 14:46:52,010 - INFO - evaluating now!
2023-12-20 14:46:59,796 - INFO - Epoch [22/100] train_loss: 21.2870, val_loss: 21.9242, lr: 0.001000, 39.08s
2023-12-20 14:46:59,806 - INFO - Saved model at 22
2023-12-20 14:46:59,806 - INFO - Val loss decrease from 22.0550 to 21.9242, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch22.tar
2023-12-20 14:47:31,086 - INFO - epoch complete!
2023-12-20 14:47:31,086 - INFO - evaluating now!
2023-12-20 14:47:38,867 - INFO - Epoch [23/100] train_loss: 21.1404, val_loss: 21.4599, lr: 0.001000, 39.06s
2023-12-20 14:47:38,878 - INFO - Saved model at 23
2023-12-20 14:47:38,879 - INFO - Val loss decrease from 21.9242 to 21.4599, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch23.tar
2023-12-20 14:48:10,163 - INFO - epoch complete!
2023-12-20 14:48:10,164 - INFO - evaluating now!
2023-12-20 14:48:17,970 - INFO - Epoch [24/100] train_loss: 21.1433, val_loss: 21.3254, lr: 0.001000, 39.09s
2023-12-20 14:48:17,989 - INFO - Saved model at 24
2023-12-20 14:48:17,990 - INFO - Val loss decrease from 21.4599 to 21.3254, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch24.tar
2023-12-20 14:48:49,244 - INFO - epoch complete!
2023-12-20 14:48:49,244 - INFO - evaluating now!
2023-12-20 14:48:57,010 - INFO - Epoch [25/100] train_loss: 20.9536, val_loss: 22.4117, lr: 0.001000, 39.02s
2023-12-20 14:49:28,295 - INFO - epoch complete!
2023-12-20 14:49:28,295 - INFO - evaluating now!
2023-12-20 14:49:36,079 - INFO - Epoch [26/100] train_loss: 20.8978, val_loss: 23.2464, lr: 0.001000, 39.07s
2023-12-20 14:50:07,383 - INFO - epoch complete!
2023-12-20 14:50:07,383 - INFO - evaluating now!
2023-12-20 14:50:15,154 - INFO - Epoch [27/100] train_loss: 20.8129, val_loss: 21.7298, lr: 0.001000, 39.07s
2023-12-20 14:50:46,438 - INFO - epoch complete!
2023-12-20 14:50:46,438 - INFO - evaluating now!
2023-12-20 14:50:54,239 - INFO - Epoch [28/100] train_loss: 20.7558, val_loss: 21.4180, lr: 0.001000, 39.09s
2023-12-20 14:51:25,545 - INFO - epoch complete!
2023-12-20 14:51:25,545 - INFO - evaluating now!
2023-12-20 14:51:33,350 - INFO - Epoch [29/100] train_loss: 20.6849, val_loss: 21.7477, lr: 0.001000, 39.11s
2023-12-20 14:52:04,658 - INFO - epoch complete!
2023-12-20 14:52:04,658 - INFO - evaluating now!
2023-12-20 14:52:12,458 - INFO - Epoch [30/100] train_loss: 20.6008, val_loss: 22.1760, lr: 0.001000, 39.11s
2023-12-20 14:52:43,754 - INFO - epoch complete!
2023-12-20 14:52:43,754 - INFO - evaluating now!
2023-12-20 14:52:51,549 - INFO - Epoch [31/100] train_loss: 20.5157, val_loss: 21.7537, lr: 0.001000, 39.09s
2023-12-20 14:53:22,810 - INFO - epoch complete!
2023-12-20 14:53:22,810 - INFO - evaluating now!
2023-12-20 14:53:30,578 - INFO - Epoch [32/100] train_loss: 20.5527, val_loss: 20.9607, lr: 0.001000, 39.03s
2023-12-20 14:53:30,588 - INFO - Saved model at 32
2023-12-20 14:53:30,588 - INFO - Val loss decrease from 21.3254 to 20.9607, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch32.tar
2023-12-20 14:54:01,859 - INFO - epoch complete!
2023-12-20 14:54:01,859 - INFO - evaluating now!
2023-12-20 14:54:09,622 - INFO - Epoch [33/100] train_loss: 20.4374, val_loss: 21.2796, lr: 0.001000, 39.03s
2023-12-20 14:54:40,876 - INFO - epoch complete!
2023-12-20 14:54:40,876 - INFO - evaluating now!
2023-12-20 14:54:48,640 - INFO - Epoch [34/100] train_loss: 20.3177, val_loss: 21.1857, lr: 0.001000, 39.02s
2023-12-20 14:55:19,867 - INFO - epoch complete!
2023-12-20 14:55:19,867 - INFO - evaluating now!
2023-12-20 14:55:27,632 - INFO - Epoch [35/100] train_loss: 20.3177, val_loss: 21.4940, lr: 0.001000, 38.99s
2023-12-20 14:55:58,877 - INFO - epoch complete!
2023-12-20 14:55:58,877 - INFO - evaluating now!
2023-12-20 14:56:06,639 - INFO - Epoch [36/100] train_loss: 20.2410, val_loss: 21.2811, lr: 0.001000, 39.01s
2023-12-20 14:56:37,886 - INFO - epoch complete!
2023-12-20 14:56:37,887 - INFO - evaluating now!
2023-12-20 14:56:45,640 - INFO - Epoch [37/100] train_loss: 20.1242, val_loss: 20.8325, lr: 0.001000, 39.00s
2023-12-20 14:56:45,651 - INFO - Saved model at 37
2023-12-20 14:56:45,651 - INFO - Val loss decrease from 20.9607 to 20.8325, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch37.tar
2023-12-20 14:57:16,922 - INFO - epoch complete!
2023-12-20 14:57:16,922 - INFO - evaluating now!
2023-12-20 14:57:24,707 - INFO - Epoch [38/100] train_loss: 20.0199, val_loss: 20.6278, lr: 0.001000, 39.06s
2023-12-20 14:57:24,717 - INFO - Saved model at 38
2023-12-20 14:57:24,718 - INFO - Val loss decrease from 20.8325 to 20.6278, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch38.tar
2023-12-20 14:57:55,991 - INFO - epoch complete!
2023-12-20 14:57:55,991 - INFO - evaluating now!
2023-12-20 14:58:03,773 - INFO - Epoch [39/100] train_loss: 20.0118, val_loss: 21.3442, lr: 0.001000, 39.06s
2023-12-20 14:58:35,090 - INFO - epoch complete!
2023-12-20 14:58:35,090 - INFO - evaluating now!
2023-12-20 14:58:42,878 - INFO - Epoch [40/100] train_loss: 19.9943, val_loss: 21.8064, lr: 0.001000, 39.11s
2023-12-20 14:59:14,108 - INFO - epoch complete!
2023-12-20 14:59:14,108 - INFO - evaluating now!
2023-12-20 14:59:21,881 - INFO - Epoch [41/100] train_loss: 19.9349, val_loss: 20.9944, lr: 0.001000, 39.00s
2023-12-20 14:59:53,154 - INFO - epoch complete!
2023-12-20 14:59:53,154 - INFO - evaluating now!
2023-12-20 15:00:00,921 - INFO - Epoch [42/100] train_loss: 19.9035, val_loss: 20.4922, lr: 0.001000, 39.04s
2023-12-20 15:00:00,931 - INFO - Saved model at 42
2023-12-20 15:00:00,931 - INFO - Val loss decrease from 20.6278 to 20.4922, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch42.tar
2023-12-20 15:00:32,206 - INFO - epoch complete!
2023-12-20 15:00:32,206 - INFO - evaluating now!
2023-12-20 15:00:40,005 - INFO - Epoch [43/100] train_loss: 19.8554, val_loss: 20.2320, lr: 0.001000, 39.07s
2023-12-20 15:00:40,016 - INFO - Saved model at 43
2023-12-20 15:00:40,016 - INFO - Val loss decrease from 20.4922 to 20.2320, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch43.tar
2023-12-20 15:01:11,345 - INFO - epoch complete!
2023-12-20 15:01:11,345 - INFO - evaluating now!
2023-12-20 15:01:19,136 - INFO - Epoch [44/100] train_loss: 19.7968, val_loss: 20.3453, lr: 0.001000, 39.12s
2023-12-20 15:01:50,464 - INFO - epoch complete!
2023-12-20 15:01:50,464 - INFO - evaluating now!
2023-12-20 15:01:58,253 - INFO - Epoch [45/100] train_loss: 19.7723, val_loss: 20.6651, lr: 0.001000, 39.12s
2023-12-20 15:02:29,572 - INFO - epoch complete!
2023-12-20 15:02:29,572 - INFO - evaluating now!
2023-12-20 15:02:37,358 - INFO - Epoch [46/100] train_loss: 19.6580, val_loss: 20.2470, lr: 0.001000, 39.10s
2023-12-20 15:03:08,697 - INFO - epoch complete!
2023-12-20 15:03:08,697 - INFO - evaluating now!
2023-12-20 15:03:16,496 - INFO - Epoch [47/100] train_loss: 19.6566, val_loss: 20.3074, lr: 0.001000, 39.14s
2023-12-20 15:03:47,771 - INFO - epoch complete!
2023-12-20 15:03:47,771 - INFO - evaluating now!
2023-12-20 15:03:55,594 - INFO - Epoch [48/100] train_loss: 19.6049, val_loss: 20.0638, lr: 0.001000, 39.10s
2023-12-20 15:03:55,606 - INFO - Saved model at 48
2023-12-20 15:03:55,606 - INFO - Val loss decrease from 20.2320 to 20.0638, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch48.tar
2023-12-20 15:04:26,993 - INFO - epoch complete!
2023-12-20 15:04:26,993 - INFO - evaluating now!
2023-12-20 15:04:34,792 - INFO - Epoch [49/100] train_loss: 19.6013, val_loss: 20.0074, lr: 0.001000, 39.19s
2023-12-20 15:04:34,803 - INFO - Saved model at 49
2023-12-20 15:04:34,803 - INFO - Val loss decrease from 20.0638 to 20.0074, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch49.tar
2023-12-20 15:05:06,063 - INFO - epoch complete!
2023-12-20 15:05:06,063 - INFO - evaluating now!
2023-12-20 15:05:13,857 - INFO - Epoch [50/100] train_loss: 19.5019, val_loss: 19.9614, lr: 0.001000, 39.05s
2023-12-20 15:05:13,869 - INFO - Saved model at 50
2023-12-20 15:05:13,869 - INFO - Val loss decrease from 20.0074 to 19.9614, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch50.tar
2023-12-20 15:05:45,142 - INFO - epoch complete!
2023-12-20 15:05:45,142 - INFO - evaluating now!
2023-12-20 15:05:52,905 - INFO - Epoch [51/100] train_loss: 19.4883, val_loss: 19.9702, lr: 0.001000, 39.04s
2023-12-20 15:06:24,171 - INFO - epoch complete!
2023-12-20 15:06:24,171 - INFO - evaluating now!
2023-12-20 15:06:31,940 - INFO - Epoch [52/100] train_loss: 19.5275, val_loss: 20.3949, lr: 0.001000, 39.04s
2023-12-20 15:07:03,169 - INFO - epoch complete!
2023-12-20 15:07:03,169 - INFO - evaluating now!
2023-12-20 15:07:10,931 - INFO - Epoch [53/100] train_loss: 19.4751, val_loss: 20.1460, lr: 0.001000, 38.99s
2023-12-20 15:07:42,182 - INFO - epoch complete!
2023-12-20 15:07:42,182 - INFO - evaluating now!
2023-12-20 15:07:49,943 - INFO - Epoch [54/100] train_loss: 19.3981, val_loss: 20.1977, lr: 0.001000, 39.01s
2023-12-20 15:08:21,209 - INFO - epoch complete!
2023-12-20 15:08:21,210 - INFO - evaluating now!
2023-12-20 15:08:28,982 - INFO - Epoch [55/100] train_loss: 19.3734, val_loss: 19.9581, lr: 0.001000, 39.04s
2023-12-20 15:08:28,993 - INFO - Saved model at 55
2023-12-20 15:08:28,993 - INFO - Val loss decrease from 19.9614 to 19.9581, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch55.tar
2023-12-20 15:09:00,223 - INFO - epoch complete!
2023-12-20 15:09:00,223 - INFO - evaluating now!
2023-12-20 15:09:08,016 - INFO - Epoch [56/100] train_loss: 19.3290, val_loss: 20.0744, lr: 0.001000, 39.02s
2023-12-20 15:09:39,242 - INFO - epoch complete!
2023-12-20 15:09:39,242 - INFO - evaluating now!
2023-12-20 15:09:46,995 - INFO - Epoch [57/100] train_loss: 19.2589, val_loss: 19.8481, lr: 0.001000, 38.98s
2023-12-20 15:09:47,006 - INFO - Saved model at 57
2023-12-20 15:09:47,006 - INFO - Val loss decrease from 19.9581 to 19.8481, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch57.tar
2023-12-20 15:10:18,225 - INFO - epoch complete!
2023-12-20 15:10:18,225 - INFO - evaluating now!
2023-12-20 15:10:25,984 - INFO - Epoch [58/100] train_loss: 19.2649, val_loss: 19.8499, lr: 0.001000, 38.98s
2023-12-20 15:10:57,190 - INFO - epoch complete!
2023-12-20 15:10:57,190 - INFO - evaluating now!
2023-12-20 15:11:04,953 - INFO - Epoch [59/100] train_loss: 19.1824, val_loss: 20.5740, lr: 0.001000, 38.97s
2023-12-20 15:11:36,149 - INFO - epoch complete!
2023-12-20 15:11:36,149 - INFO - evaluating now!
2023-12-20 15:11:43,923 - INFO - Epoch [60/100] train_loss: 19.2070, val_loss: 20.0436, lr: 0.001000, 38.97s
2023-12-20 15:12:15,169 - INFO - epoch complete!
2023-12-20 15:12:15,170 - INFO - evaluating now!
2023-12-20 15:12:22,932 - INFO - Epoch [61/100] train_loss: 19.1223, val_loss: 20.0945, lr: 0.001000, 39.01s
2023-12-20 15:12:54,189 - INFO - epoch complete!
2023-12-20 15:12:54,189 - INFO - evaluating now!
2023-12-20 15:13:01,969 - INFO - Epoch [62/100] train_loss: 19.1486, val_loss: 19.6077, lr: 0.001000, 39.04s
2023-12-20 15:13:01,980 - INFO - Saved model at 62
2023-12-20 15:13:01,980 - INFO - Val loss decrease from 19.8481 to 19.6077, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch62.tar
2023-12-20 15:13:33,264 - INFO - epoch complete!
2023-12-20 15:13:33,264 - INFO - evaluating now!
2023-12-20 15:13:41,024 - INFO - Epoch [63/100] train_loss: 19.1145, val_loss: 20.0708, lr: 0.001000, 39.04s
2023-12-20 15:14:12,270 - INFO - epoch complete!
2023-12-20 15:14:12,270 - INFO - evaluating now!
2023-12-20 15:14:20,074 - INFO - Epoch [64/100] train_loss: 19.1347, val_loss: 19.5825, lr: 0.001000, 39.05s
2023-12-20 15:14:20,085 - INFO - Saved model at 64
2023-12-20 15:14:20,085 - INFO - Val loss decrease from 19.6077 to 19.5825, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch64.tar
2023-12-20 15:14:51,372 - INFO - epoch complete!
2023-12-20 15:14:51,373 - INFO - evaluating now!
2023-12-20 15:14:59,156 - INFO - Epoch [65/100] train_loss: 19.0750, val_loss: 19.9945, lr: 0.001000, 39.07s
2023-12-20 15:15:30,414 - INFO - epoch complete!
2023-12-20 15:15:30,414 - INFO - evaluating now!
2023-12-20 15:15:38,164 - INFO - Epoch [66/100] train_loss: 19.0259, val_loss: 20.0243, lr: 0.001000, 39.01s
2023-12-20 15:16:09,375 - INFO - epoch complete!
2023-12-20 15:16:09,376 - INFO - evaluating now!
2023-12-20 15:16:17,128 - INFO - Epoch [67/100] train_loss: 19.0273, val_loss: 19.7966, lr: 0.001000, 38.96s
2023-12-20 15:16:48,323 - INFO - epoch complete!
2023-12-20 15:16:48,323 - INFO - evaluating now!
2023-12-20 15:16:56,093 - INFO - Epoch [68/100] train_loss: 18.9891, val_loss: 19.6529, lr: 0.001000, 38.96s
2023-12-20 15:17:27,334 - INFO - epoch complete!
2023-12-20 15:17:27,334 - INFO - evaluating now!
2023-12-20 15:17:35,100 - INFO - Epoch [69/100] train_loss: 18.9934, val_loss: 19.5792, lr: 0.001000, 39.01s
2023-12-20 15:17:35,110 - INFO - Saved model at 69
2023-12-20 15:17:35,110 - INFO - Val loss decrease from 19.5825 to 19.5792, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch69.tar
2023-12-20 15:18:06,424 - INFO - epoch complete!
2023-12-20 15:18:06,424 - INFO - evaluating now!
2023-12-20 15:18:14,221 - INFO - Epoch [70/100] train_loss: 18.9630, val_loss: 20.2502, lr: 0.001000, 39.11s
2023-12-20 15:18:45,529 - INFO - epoch complete!
2023-12-20 15:18:45,529 - INFO - evaluating now!
2023-12-20 15:18:53,319 - INFO - Epoch [71/100] train_loss: 18.9188, val_loss: 20.2482, lr: 0.001000, 39.10s
2023-12-20 15:19:24,640 - INFO - epoch complete!
2023-12-20 15:19:24,640 - INFO - evaluating now!
2023-12-20 15:19:32,418 - INFO - Epoch [72/100] train_loss: 18.9013, val_loss: 19.6176, lr: 0.001000, 39.10s
2023-12-20 15:20:03,653 - INFO - epoch complete!
2023-12-20 15:20:03,653 - INFO - evaluating now!
2023-12-20 15:20:11,427 - INFO - Epoch [73/100] train_loss: 18.8827, val_loss: 19.3762, lr: 0.001000, 39.01s
2023-12-20 15:20:11,438 - INFO - Saved model at 73
2023-12-20 15:20:11,438 - INFO - Val loss decrease from 19.5792 to 19.3762, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch73.tar
2023-12-20 15:20:42,655 - INFO - epoch complete!
2023-12-20 15:20:42,655 - INFO - evaluating now!
2023-12-20 15:20:50,410 - INFO - Epoch [74/100] train_loss: 18.8747, val_loss: 19.5771, lr: 0.001000, 38.97s
2023-12-20 15:21:21,661 - INFO - epoch complete!
2023-12-20 15:21:21,661 - INFO - evaluating now!
2023-12-20 15:21:29,443 - INFO - Epoch [75/100] train_loss: 18.8938, val_loss: 19.3287, lr: 0.001000, 39.03s
2023-12-20 15:21:29,454 - INFO - Saved model at 75
2023-12-20 15:21:29,454 - INFO - Val loss decrease from 19.3762 to 19.3287, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch75.tar
2023-12-20 15:22:00,771 - INFO - epoch complete!
2023-12-20 15:22:00,771 - INFO - evaluating now!
2023-12-20 15:22:08,583 - INFO - Epoch [76/100] train_loss: 18.8816, val_loss: 19.4782, lr: 0.001000, 39.13s
2023-12-20 15:22:39,891 - INFO - epoch complete!
2023-12-20 15:22:39,891 - INFO - evaluating now!
2023-12-20 15:22:47,679 - INFO - Epoch [77/100] train_loss: 18.8719, val_loss: 19.5196, lr: 0.001000, 39.10s
2023-12-20 15:23:18,970 - INFO - epoch complete!
2023-12-20 15:23:18,970 - INFO - evaluating now!
2023-12-20 15:23:26,767 - INFO - Epoch [78/100] train_loss: 18.8453, val_loss: 19.4032, lr: 0.001000, 39.09s
2023-12-20 15:23:58,044 - INFO - epoch complete!
2023-12-20 15:23:58,044 - INFO - evaluating now!
2023-12-20 15:24:05,819 - INFO - Epoch [79/100] train_loss: 18.7994, val_loss: 19.6640, lr: 0.001000, 39.05s
2023-12-20 15:24:37,055 - INFO - epoch complete!
2023-12-20 15:24:37,055 - INFO - evaluating now!
2023-12-20 15:24:44,844 - INFO - Epoch [80/100] train_loss: 18.7541, val_loss: 19.8738, lr: 0.001000, 39.02s
2023-12-20 15:25:16,101 - INFO - epoch complete!
2023-12-20 15:25:16,101 - INFO - evaluating now!
2023-12-20 15:25:23,875 - INFO - Epoch [81/100] train_loss: 18.7774, val_loss: 19.2778, lr: 0.001000, 39.03s
2023-12-20 15:25:23,886 - INFO - Saved model at 81
2023-12-20 15:25:23,886 - INFO - Val loss decrease from 19.3287 to 19.2778, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch81.tar
2023-12-20 15:25:55,157 - INFO - epoch complete!
2023-12-20 15:25:55,157 - INFO - evaluating now!
2023-12-20 15:26:02,930 - INFO - Epoch [82/100] train_loss: 18.7101, val_loss: 19.7233, lr: 0.001000, 39.04s
2023-12-20 15:26:34,189 - INFO - epoch complete!
2023-12-20 15:26:34,189 - INFO - evaluating now!
2023-12-20 15:26:41,994 - INFO - Epoch [83/100] train_loss: 18.7240, val_loss: 19.3698, lr: 0.001000, 39.06s
2023-12-20 15:27:13,297 - INFO - epoch complete!
2023-12-20 15:27:13,297 - INFO - evaluating now!
2023-12-20 15:27:21,084 - INFO - Epoch [84/100] train_loss: 18.7478, val_loss: 19.4579, lr: 0.001000, 39.09s
2023-12-20 15:27:52,363 - INFO - epoch complete!
2023-12-20 15:27:52,363 - INFO - evaluating now!
2023-12-20 15:28:00,125 - INFO - Epoch [85/100] train_loss: 18.6349, val_loss: 19.3476, lr: 0.001000, 39.04s
2023-12-20 15:28:31,399 - INFO - epoch complete!
2023-12-20 15:28:31,399 - INFO - evaluating now!
2023-12-20 15:28:39,196 - INFO - Epoch [86/100] train_loss: 18.6687, val_loss: 19.6396, lr: 0.001000, 39.07s
2023-12-20 15:29:10,450 - INFO - epoch complete!
2023-12-20 15:29:10,450 - INFO - evaluating now!
2023-12-20 15:29:18,234 - INFO - Epoch [87/100] train_loss: 18.6865, val_loss: 19.2355, lr: 0.001000, 39.04s
2023-12-20 15:29:18,245 - INFO - Saved model at 87
2023-12-20 15:29:18,245 - INFO - Val loss decrease from 19.2778 to 19.2355, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch87.tar
2023-12-20 15:29:49,546 - INFO - epoch complete!
2023-12-20 15:29:49,546 - INFO - evaluating now!
2023-12-20 15:29:57,336 - INFO - Epoch [88/100] train_loss: 18.6770, val_loss: 19.5920, lr: 0.001000, 39.09s
2023-12-20 15:30:28,622 - INFO - epoch complete!
2023-12-20 15:30:28,623 - INFO - evaluating now!
2023-12-20 15:30:36,419 - INFO - Epoch [89/100] train_loss: 18.6170, val_loss: 19.4947, lr: 0.001000, 39.08s
2023-12-20 15:31:07,668 - INFO - epoch complete!
2023-12-20 15:31:07,668 - INFO - evaluating now!
2023-12-20 15:31:15,435 - INFO - Epoch [90/100] train_loss: 18.6037, val_loss: 19.2221, lr: 0.001000, 39.02s
2023-12-20 15:31:15,447 - INFO - Saved model at 90
2023-12-20 15:31:15,447 - INFO - Val loss decrease from 19.2355 to 19.2221, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch90.tar
2023-12-20 15:31:46,715 - INFO - epoch complete!
2023-12-20 15:31:46,715 - INFO - evaluating now!
2023-12-20 15:31:54,484 - INFO - Epoch [91/100] train_loss: 18.5729, val_loss: 19.4000, lr: 0.001000, 39.04s
2023-12-20 15:32:25,753 - INFO - epoch complete!
2023-12-20 15:32:25,753 - INFO - evaluating now!
2023-12-20 15:32:33,520 - INFO - Epoch [92/100] train_loss: 18.5510, val_loss: 19.4550, lr: 0.001000, 39.04s
2023-12-20 15:33:04,752 - INFO - epoch complete!
2023-12-20 15:33:04,752 - INFO - evaluating now!
2023-12-20 15:33:12,514 - INFO - Epoch [93/100] train_loss: 18.5231, val_loss: 19.5388, lr: 0.001000, 38.99s
2023-12-20 15:33:43,725 - INFO - epoch complete!
2023-12-20 15:33:43,725 - INFO - evaluating now!
2023-12-20 15:33:51,487 - INFO - Epoch [94/100] train_loss: 18.5792, val_loss: 19.2789, lr: 0.001000, 38.97s
2023-12-20 15:34:22,697 - INFO - epoch complete!
2023-12-20 15:34:22,698 - INFO - evaluating now!
2023-12-20 15:34:30,452 - INFO - Epoch [95/100] train_loss: 18.5087, val_loss: 19.6269, lr: 0.001000, 38.96s
2023-12-20 15:35:01,736 - INFO - epoch complete!
2023-12-20 15:35:01,736 - INFO - evaluating now!
2023-12-20 15:35:09,538 - INFO - Epoch [96/100] train_loss: 18.5098, val_loss: 19.2117, lr: 0.001000, 39.09s
2023-12-20 15:35:09,548 - INFO - Saved model at 96
2023-12-20 15:35:09,548 - INFO - Val loss decrease from 19.2221 to 19.2117, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch96.tar
2023-12-20 15:35:40,840 - INFO - epoch complete!
2023-12-20 15:35:40,840 - INFO - evaluating now!
2023-12-20 15:35:48,609 - INFO - Epoch [97/100] train_loss: 18.5292, val_loss: 19.4701, lr: 0.001000, 39.06s
2023-12-20 15:36:19,877 - INFO - epoch complete!
2023-12-20 15:36:19,877 - INFO - evaluating now!
2023-12-20 15:36:27,655 - INFO - Epoch [98/100] train_loss: 18.5219, val_loss: 20.0903, lr: 0.001000, 39.05s
2023-12-20 15:36:58,912 - INFO - epoch complete!
2023-12-20 15:36:58,912 - INFO - evaluating now!
2023-12-20 15:37:06,679 - INFO - Epoch [99/100] train_loss: 18.4861, val_loss: 19.1846, lr: 0.001000, 39.02s
2023-12-20 15:37:06,689 - INFO - Saved model at 99
2023-12-20 15:37:06,689 - INFO - Val loss decrease from 19.2117 to 19.1846, saving to ./libcity/cache/33958/model_cache/GWNET_PeMS04_epoch99.tar
2023-12-20 15:37:06,689 - INFO - Trained totally 100 epochs, average train time is 31.278s, average eval time is 7.780s
2023-12-20 15:37:06,700 - INFO - Loaded model at 99
2023-12-20 15:37:06,700 - INFO - Saved model at ./libcity/cache/33958/model_cache/GWNET_PeMS04.m
2023-12-20 15:37:06,711 - INFO - Start evaluating ...
2023-12-20 15:37:16,367 - INFO - Note that you select the single mode to evaluate!
2023-12-20 15:37:16,369 - INFO - Evaluate result is saved at ./libcity/cache/33958/evaluate_cache/2023_12_20_15_37_16_GWNET_PeMS04.csv
2023-12-20 15:37:16,373 - INFO - 
          MAE  MAPE          MSE       RMSE  masked_MAE  masked_MAPE   masked_MSE  masked_RMSE        R2      EVAR
1   16.938581   inf   771.741516  27.780235   16.851761     0.113991   727.982117    26.981144  0.968999  0.969014
2   17.625540   inf   859.793335  29.322233   17.465054     0.120350   785.043457    28.018627  0.965457  0.965457
3   18.163782   inf   918.268982  30.302954   17.974998     0.122897   829.506042    28.801147  0.963111  0.963111
4   18.555670   inf   961.191833  31.003094   18.354300     0.126317   864.914917    29.409435  0.961383  0.961385
5   18.900755   inf   997.794434  31.587885   18.687111     0.129203   894.998352    29.916523  0.959929  0.959930
6   19.181421   inf  1021.172424  31.955788   18.967403     0.130770   919.463562    30.322657  0.958992  0.958993
7   19.509636   inf  1049.211792  32.391541   19.296144     0.133716   946.474670    30.764828  0.957888  0.957888
8   19.792646   inf  1075.236938  32.790806   19.577555     0.135573   971.175659    31.163691  0.956840  0.956843
9   20.071682   inf  1096.813354  33.118172   19.864759     0.137146   997.149719    31.577677  0.955971  0.955972
10  20.336903   inf  1122.314697  33.500965   20.126307     0.141009  1021.268555    31.957293  0.954976  0.954982
11  20.663731   inf  1145.643555  33.847355   20.461746     0.143314  1045.371582    32.332207  0.954064  0.954079
12  21.033249   inf  1177.011963  34.307610   20.827541     0.146040  1075.569458    32.795876  0.952839  0.952870
